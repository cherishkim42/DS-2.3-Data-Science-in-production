{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MAP RUNNING TIME\n",
    "\n",
    "https://github.com/Make-School-Courses/DS-2.3-Data-Science-in-production/blob/master/Lessons/Pyspark_Notebooks/Pyspark_array_manipulation_1.ipynb\n",
    "\n",
    "The first line defines a base RDD from an external file. This dataset is not loaded in memory or otherwise acted on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# .... UHHH MORE STUFF! (beyond what's in the linked ntbk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70709.97100833799\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from pyspark import SparkContext\n",
    "# sc = SparkContext()\n",
    "\n",
    "nums = sc.parallelize(range(100000), numSlices=100)\n",
    "doubled = nums.map(lambda n: n*2)\n",
    "total = doubled.filter(lambda n: n%4==0).reduce(lambda a,b: a+b)\n",
    "print(math.sqrt(total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70709.97100833799\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def f(ls):\n",
    "    s = 0\n",
    "    for i in ls:\n",
    "        if (i*2)%4 == 0:\n",
    "            s += (i*2)\n",
    "    return math.sqrt(s)\n",
    "\n",
    "print(f(range(100000)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No need for .collect! Everything will be distributed, computation will happen, yay.\n",
    "\n",
    "(I have no clue what any of that means. That's just what Milad said)\n",
    "\n",
    "So the only difference between using spark and python native is here, nums c=will come after function, or doubled will be AFTER the condition (lambda n: n%4==0) -- _Cherish says: So the ordering of things is different_\n",
    "\n",
    "<b>Map reduce filter is the ONLY way when you use Pyspark. I think? God I am so lost someone help pls</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Difference between map and flatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[range(0, 1), range(0, 2), range(0, 3), range(0, 4)]\n",
      "[0, 0, 1, 0, 1, 2, 0, 1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import pyspark\n",
    "from pyspark import SparkContext\n",
    "\n",
    "sc = SparkContext()\n",
    "\n",
    "values = sc.parallelize([1, 2, 3, 4], 2)\n",
    "print(values.map(range).collect())\n",
    "print(values.flatMap(range).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
